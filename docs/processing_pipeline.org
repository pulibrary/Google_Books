#+title: Designing a Pipeline for Copying Books from GRIN
#+date: <2025-04-17 Thu>
#+author: Cliff Wulfman
#+email: cwulfman@princeton.edu

PUL wants to copy all of its books digitized during the Google Books project to a server under its control.  This document proposes a method for accomplishing this goal.


* General Considerations
** Scale
GRIN says PUL has 277,970 books Google Books. Harvard says the default conversion rate (how many books GRIN can process into downloadable tarballs) is about 5,000 per day. Assuming we can download tarballs as fast as GRIN can put them up, that means a minimum of 56 days to process everything.

The size of the _converted pool at Google Books is also 5,000: once the pool size reaches 5000, older packages are replaced with new ones in the pool. 

* Existing Resources
Google has written a [[https://docs.google.com/document/d/1ugKUSkq4jAwmyWu3HubUIobQA1ag4VgRP1JjLeGUW20/edit?usp=sharing][GRIN Overview]] that describes the Google Return INterface (GRIN) and sample Python code that uses OAuth2 to access the GRIN API.  Harvard's =grin-to-s3= package (below)  builds on this script.


** grin-to-s3
The Institutional Data Initiative at Harvard has developed a set of Python scripts called =grin-to-s3= that they use to migrate their books.  Some of its features are local to Harvard or not relevant to us, but it provides a model, some modules, and some guidance that are useful to us.

The suite includes scripts for hitting the GRIN API; matching GRIN barcodes to MARC records; moving an entire corpus from GRIN to s3 and doing things with the retrieved archives; and tools for generating data sets for machine learning tasks.

Harvard implements a pipeline in a script called =liberate.py=. 

It assumes you have already:

- Downloaded a list of GB barcodes (using instadin's =grin.py= script)  (_all_files.tsv)
- Downloaded the latest MARCXML dump from Stanford's POD aggregator (latest was princeton-2023-02-10-full-marcxml.xml)
- Used process_pod_xml.py to match all the Google barcodes against POD records and augment them (records in grin_processed/pod_xml)

=Liberate.py= does much more than we want to do at the moment:
#+begin_quote
Processing does a few things:

1. Decrypt and unpack each `.tar.gz.gpg` archive
2. Extract and upload `LIBRARY_BARCODE.xml` (as `BARCODE.xml`)
3. Compile the page-by-page text OCR files into one .txt file, `BARCODE.txt` and as a tarball of each page as a single file, `BARCODE.txt.tar.gz`.  Both of these are uploaded to s3.
4. If POD XML file(s) exists within `--output_dir`'s `pod_xml` subdirectory, upload any matching files as `BARCODE.pod.xml`.
5. If a MODS XML file can be found in Harvards Library Cloud, upload that.
6. As data is collected and put into S3, a `.retrieval` sister file containing a timestamp is uploaded.
#+end_quote

Harvard's =liberate= script can serve as a model for our implementation, but it cannot be used out-of-the box, for several reasons:

- it includes functionality we don't want;
- it bundles functionality into a single script: the pipeline is contained within the script, making the pipeline single-threaded, resulting in a slower, more fragile process.


* Design 1: An Asynchronous Pipeline

A better approach might be to implement the pipeline as a collection of asynchronous processes or microservices that either communicate via a messaging queue (like RabbitMQ) or through polling. Such a pipeline would be more robust (if one process fails the entire pipeline doesn't stop); faster (multiple processes running at the same time); and easier to maintain (proper separation of concerns).  Mosts importantly, it would be configurable and extensible: an OCR process, for example, could be inserted into the pipeline without modifying anything.

The processes in the pipeline must communicate with one another.  Two possible ways:

- passing tokens through a series of buckets (directories), Kanban style
- using a message broker like RabbitMQ

We should consider both.

There are at least three processor classes: a staging process; a downloading process, and an uploading process. A fourth process, a monitor, could be used gather performance stats.

** Staging Process
This process selects Google Book resources to work on, according to some criteria. One strategy might be to feed it a list of barcodes (slices of the _all_books.tsv file, for example).  This process can be throttled and paused.

In the Kanban model, the Stager is given a master list of barcodes and the location of a Staging Bucket on startup. From this list, the Stager picks a number of barcodes (this number can be adjusted) and drops tokens for them in the Staging Bucket. These tokens are files containg the barcode and, possibly, other information (like an updated MARC record).  It then monitors the Staging Bucket; when it is empty, it selects another tranche.  When the list is empty, it stops.

In the message-broker model, a Queue is passed to the Stager at startup. The Stager posts message to the Queue.

** Downloading Process
The Downloader downloads files with the GRIN API. On start-up, it should be given the necessary credentials, the location of a Staging Bucket, and the location of a Processing Bucket. 

In the Kanban model, the Downloader polls the Staging Bucket.  When there are tokens in it, the Downloader picks one and puts a lock on it, so other Downloaders don't process it (it could simply give it a different extension, for example).  It downloads the encrypted tarball from GRIN, puts it in the Processing Bucket; it moves the locked token to the Processing Bucket as well and then unlocks it.

In the message-broker model, two Queues are passed to the Downloader at startup: a Staged Queue and a To-Be-Processed Queue; the Downloader takes a message from the queue, downloads the encrypted tarball and stores it; then puts a message on the TBP Queue.

** [Processing Processes]
At this point in the pipeline, operations can be performed on the book before it is uploaded to the cloud store.  At Harvard, they replace the Google's MARC record with an updated one from the Stanford POD; we might do the same.  Google's tarball contains .jp2, .txt, and .html (hocr) files; we might want to run a different OCR engine over the pages, or do some NLP analysis; Hathi does this. Or we do nothing other than move the item to a ready-to-be-uploded bucket.

** Uploading Process
The Uploader monitors a ready-to-be-uploaded bucket for tarballs; when it finds one, it uploads the tarball a cloud server (an Uploader is configured for a particular service), and then moves the tarball to a Completed bucket.

** Cleanup Process
This process (or processes) monitors the Completed bucket and takes some action or actions: it may write a report, or write to a log; if the bucket reaches a certain size, it empties it.

